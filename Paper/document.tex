\documentclass[journal,onecolumn]{IEEEtran}

\makeatletter
\def\endthebibliography{%
	\def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
	\endlist
}
\makeatother
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{float}


\usepackage[utf8]{inputenc}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}



% Python style for highlighting
\newcommand\pythonstyle{\lstset{
		language=Python,
		basicstyle=\ttm,
		morekeywords={self},              % Add keywords here
		keywordstyle=\ttb\color{deepblue},
		emph={MyClass,__init__},          % Custom highlighting
		emphstyle=\ttb\color{deepred},    % Custom highlighting style
		stringstyle=\color{deepgreen},
		frame=none,                         % Any extra options here
		showstringspaces=false
}}

% Python environment
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{#1}
}
{}

\usepackage{graphicx}
\graphicspath{ {./Material/pictures/} }




% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
	%
	% paper title
	% Titles are generally capitalized except for words such as a, an, and, as,
	% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
	% not capitalized unless they are the first or last word of the title.
	% Linebreaks \\ can be used within to get better formatting as desired.
	% Do not put math or special symbols in the title.
	\title{Distance and Movement Measurement of an Object\\ based on Stereo Images}

	\author{Lennard~Rose,~5110000,~FHWS
		Moritz~Zeitler,~5118094,~FHWS% <-this % stops a space
	}
	% The paper headers
	\markboth{Seminar Smart Systems}%
	{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

	% make the title area
	\maketitle

	% As a general rule, do not put math, special symbols or citations
	% in the abstract or keywords.
	\begin{abstract}
		\noindent
		This paper explains how to compute distance to, and movement of an object within vision of a stereo camera.
		Included is the whole workflow consisting of all steps that have to be made to get such a recognition to work. These steps are the hardware setup, all calibration steps, object detection, object tracking and the measurements of the values themselves.
	\end{abstract}


	% For peer review papers, you can put extra information on the cover
	% page as needed:
	% \ifCLASSOPTIONpeerreview
	% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
	% \fi
	%
	% For peerreview papers, this IEEEtran command inserts a page break and
	% creates the second title. It will be ignored for other modes.
	\IEEEpeerreviewmaketitle

	\section{Introduction}
\label{sec:einfuehrung}
\noindent
From a high level standpoint the human eye is a very simple complex. It enables us to see things, even in color. But if we go deeper there is much more to our sight. For example we can see depth, which is very complex as a concept. This is based on the human eye having to different point-of-views. In this project the so called Stereo-Vision, will be translated to a computational level. To establish this we use a stereo camera setup to take photos/videos, create a depth map and try to compute the distance and movement of an object. The following chapters will describe this in a step by step fashion, starting with the theory behind it and the implementation afterwards.
\section{Stereoscopy}
\noindent
When capturing a 3D object in an image, the objects gets projected from a 3D space to a 2D (planar) projection space, losing all its depth information. This is called the Planar Projection. The question now is, how to get back the depth information from this 2D space.
With 2 Cameras, this is possible by comparing the left and the right image, called Triangulation.
On a abstract level, if one takes a picture with two cameras both of the x axes are exactly on the same level (depending on the mounting). For points on the y axes this isn't certain. The solve this problem there is the possibility to use epipolar geometry and stereoscopy.
Basically the search space gets reduced.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{epipolarGeometry.png}
	\caption{Epipolar geometry}
\end{figure}
This is done by projecting the line LL between the points OL(left camera), XL(projection of point X in the left frame) and X(being the target 3D point) as ECR on our right picture, crossing the point XR(projection of point X in the right frame). The same on the left picture, project the line LR between the points OR(right camera), XR and X, crossing the point XL as ECL. The centers of both cameras get also project to there significant other frame.
With this, the searchspace got reduced of a corresponding point to compute our disparity from to a single line, ECL respectively ECR (this is called an epipolar constraint).EL and ER is the projection of one camera onto its significant other cameras pov. These points are called epipoles. Together with LL and LR this baseline forms the epipolar plane (blue in figure 5). All this information can be represented by the Fundamental matrix of the image, which can be calculated with the projected points and the projection matrices of the cameras. Further mathematical description can be found in the book \emph{Multiple View Geometry in Computer Vision}\cite{hartley_zisserman_2004}, but won't be discussed any further at this point.
\section{Workflow}
\noindent
This chapter contains a detailed theoretical and practical description of each individual step in the workflow.
\subsection{Stereo Camera Calibration}
\noindent
Cameras are projecting a three dimensional object to a two dimensional image. This projection is never alone determined by the original object, but also by all parameters of the used camera. These you can separate in two groups, external and internal. External parameters are the camera orientation in different planes. Internal parameters are determined by the cameras internal characteristics.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{intrinsicMatrix.png}
	\caption{Intrinsic Matrix}
\end{figure}
\noindent
We can represent these internal parameters as an upper triangular matrix K. This matrix has the values fx and fy as the x and y focal lengths. cx and cy as the x and y coordinates of the optical center in the image plane. Gamma is the distortion between the axes, this value is replaced by a "0" in OpenCVÂ´s calibration function.\newline
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{extrinsicMatrix.png}
	\caption{Extrinsic Matrix}
\end{figure}
\noindent
The external parameters are represented by an 3x3 rotation matrix, that indicates the rotation bias of the camera, combined with an 3x1 translation vector.\newline
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{projectionMatrix.png}
	\caption{Projection Matrix}
\end{figure}
\noindent
Together these form the 3x4 Projection Matrix. This matrix and the 3D-coordinates of our point is used to calculate the image coordinates of the 2D projection. Calibration does nothing else than alter these matrices so our 2D projection matches the 3D object. This is usually done by giving the calibrating program/ function a set of 2D images with known 3D coordinates.\newline
This Calibration consists of two steps removing of Distortion as well as rectifying the images\cite{ImageRectification}.
\subsubsection{Distortion}
\noindent
A camera lens always has some kind of curvature. This curvature is also visible in an image taken with this lens. This is called distortion. There are many different types of distortion which can be seen in figure 4.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{distortion.jpg}
	\caption{Distortion types}
\end{figure}
\noindent
To remove this distortion we take pictures of a chessboard. OpenCV, the package we use to do all calibration steps, can easily detect key points using this chessboard pattern. This is based on the big difference in pixel values between white and black. In combination with the size of the squares as well as the amount of the squares, OpenCV is able to create a calibration matrix removing the distortion.\newline
A very important part is to be very precise while taking the calibration pictures. To get a good result you need to take calibration photos, basically of all parts of the pov of the camera.\newline
The python implementation looks like the following:
First of all we convert the input picture to grayscale and feed it into a detection function for chessboard corners.
\begin{python}
	ret, corners = cv2.findCirclesGrid(inputImage,
					   (chessboardColumns, chessboardRows),
					   None)
\end{python}
As input we have the chessboard picture, columns of the chessboard, rows of the chessboard and an output array we don't use.
This function results in a return value whether a chessboard got detected and if so the 2D points of the chessboard corners. If the detection was successful the returned points get even more refined using:
\begin{python}
	cv2.cornerSubPix(inputImage,
			 corners,
			 (11, 11),
			 (-1, -1),
			 criteria)
\end{python}
The 'cornerSubPix' of openCV refines the corner locations \cite{forstner} of the checkboard.
All results of  the previous steps are saved in a list to cache them for now. With these lists we can calibrate a single camera:
\begin{python}
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(object_pts,
						   img_pts,
						   img.shape[::-1], None, None)
\end{python}
Feeding these points as well as the shape of the image into the calibrate Camera function delivers the intrinsic and extrinsic parameter.
These parameters are stored inside the mtx variable. Also we get the overall RMS re-projection error in ret and the distortion coefficients. Rvecs\cite{DBLP} and tvecs are rotational and translation vectors, which aren't used in this system and won't be described further.
On a high level openCV executes the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that is, the total sum of squared distances between the observed feature points imagePoints and the projected (using the current estimates for camera parameters and the poses) object points objectPoints\cite{calibrateCamera}.
With these matrices we can now compute a new optimal matrix based on these parameters.\newpage
\begin{python}
h, w = img.shape[:2]
new_mtx, roi = cv2.getOptimalNewCameraMatrix(mtx,
				   	     dist,
				   	     (w, h), 1, (w, h))
\end{python}
As a result the function returns a new optimal intrinsic camera matrix. The function also returns a valid pixel region which outlines a region of all-good-pixels in the undistorted image. These matrices and results will be later used to undistort and rectify images coming from our camera for better results while computing the disparity and depth map.
\subsubsection{Rectification}
\noindent
With image rectification, the goal is to project the two different images taken by the camera onto a common image plane. Through this process we make sure that the individual lines of both images are on the same level, speaking on a very high level. This means the epipolarlines are calibrated to be exactly horizontal, which helps finding the correspondence of a point in both of the stereo pictures.
For this step we need the previous calibration matrices for both of our cameras as well as pictures from both cameras at a similar time containing the chessboard to compute the rectification. Through this computation we get:
\begin{enumerate}
	\item essential matrix
	\item rotational matrix
	\item translation vector (previously described as extrinsic matrix)
\end{enumerate}

\begin{python}
retS, new_mtxL, distL, new_mtxR, distR, Rot, Trns, Emat, Fmat = cv2.stereoCalibrate(obj_pts,
									img_ptsL,
									img_ptsR,
									mtxL,
									distL,
									mtxR,
									distR,
									imgR.shape[::-1],
									criteria_stereo,
									flags)
\end{python}
The function can also compute the full calibration for each of the cameras but isn't recommended to do so due to the high dimensionality of the parameter space and noise in the input data\cite{stereoCalibrate}. Because of this the individual calibration is done beforehand and gets supplied as an input.
With these matrices we are able to compute a coordinate representation of a 3D point in one of the cameras using the result matrices as well as the one of the individual camera matrices.
Similarly to calibrateCamera, the function minimizes the total re-projection error for all the points in all the available views from both cameras. The function returns the final value of the re-projection error.
Now the goal is to rectify these cameras using stereoRectify:
\begin{python}
rect_l, rect_r, proj_mat_l, proj_mat_r, Q, roiL, roiR = cv2.stereoRectify(new_mtxL,
										distL,
										new_mtxR,
										distR,
										imgL.shape[::-1],
										Rot,
										Trns,
										rectify_scale, (0, 0))
\end{python}
Once again all previous results are input for the function. The function results in two rectification transform matrices, which brings points given in the unrectified camera's coordinate system to points in the rectified camera's coordinate system. Also the projection matrix, in the now recitfied coordinate system of the camera, which projects points into the camera image.
\subsection{Disparity Map}
\noindent
Disparity is usually computed as a shift of an image feature when viewed in the right/left image. For example, a single point that appears at the x coordinate k (measured in pixels) in the left image may be present at the k coordinate k minus 3 in the right image. This would result in a disparity value of 3 in pixels. These disparity values are the base for the computation of the depth map.
This can be done using specific algorithms, in this case the Block Matching Algorithm is used.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{bma.png}
	\caption{Block Matching Algorithm}
\end{figure}
\noindent
Figure 6 displays the method the algorithm implements. Input is one pixel, or in this case a small window of pixels, which makes the algorithm more resilient. With the input the algorithm searches the most similar neighbor in the corresponding stereo picture. In this case the right image is the input, with the green square the search window. The algorithm searches for its corresponding window on the left picture. The best match is visualized by the green window the second best by the red window.
On a theoretical level the algorithm searches through the image line by line to find the best match.
This can be done in with different metrics, for example absolute or normalized differences. The algorithm searches for the least differences to determine correspondence. However, the algorithm tends to find multiple pairs of corresponding points due to repeating textures or similar occurrences.
In python the algorithm is implemented as a class which needs to be instantiated. To be more precise the implementation used in this project uses a slightly more advanced\cite{SGBM} but follows the same principle.
\begin{python}
stereo = cv2.StereoSGBM_create(minDisparity=min_disp,
			       numDisparities=num_disp,
			       blockSize=window_size,
			       uniquenessRatio=10,
			       speckleWindowSize=100,
			       speckleRange=32,
			       disp12MaxDiff=5,
			       P1=8*3*window_size**2,
			       P2=32*3*window_size**2)
\end{python}
Important are the different options we supply to the SGBM class. The block size parameter supplies the window size for the algorithm. The disparity amount, which has a minimum and a maximum, specifies at which range a disparity match is acceptable. Speckleand P1 and P2 are filtering algorithms to smooth the output\cite{stereoSGBM}, again we skip further explanation.
To further improve those parameters, an user interface was build to change the values on the fly.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{calibrateUI.png}
	\caption{Disparity Calibration}
\end{figure}
\noindent
In the UI are many different parameters to tune. Also there is the option to save the parameters, it isn't currently possible to use a button in openCV thus there is a slider which produces a config.json.The result can look something like Figure 8.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{disparityMap.png}
	\caption{Disparity Map}
\end{figure}
\noindent
On the left the disparity map and on the right the original left side image.
\subsection{Depth Map}
\noindent
At this point there are two different options to compute a depth value from the previously created disparity map. The first one being the usage of the openCV function 'reprojectImageTo3D', which takes the disparity map as well as the Q matrix we computed during the rectification process to compute a depth map.
Alternatively there is the possibility to sort of calibrate the disparity-depth mapping. This means taking a video/ picture of an object at a known distance, creating the corresponding disparity map and extract the value. With both of these values a regression line gets computed. With the formular of the regression line we can now average the distance of an object based on the its disparity value.
\subsection{Object Detection}
\noindent
\subsection{Object Tracking}
\noindent
\section{Measurements}
\noindent
\subsection{Distance}
\noindent
For testing purposes the initial idea is an event based system. On double-click an openCV event gets triggered, containing a x and y position of the mouse click. These values get enhanced to a window, all disparity values in this window get added up to compute an average a
\subsection{Direction}
\noindent
\section{Prerequisites}
\noindent
\section{Results}
\noindent
\section{Conclusion}
\noindent
\appendices

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{bibtex/bib/IEEEabrv.bib,bibtex/bib/IEEEexample.bib}{}
\bibliographystyle{IEEEtran}

%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\end{document}